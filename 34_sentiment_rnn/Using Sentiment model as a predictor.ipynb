{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vocab_to_int,  size: 74072\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "if os.path.exists('vocab_to_int.json'):\n",
    "    vocab_to_int = json.load(open('vocab_to_int.json', 'r'))\n",
    "    print('loaded vocab_to_int,  size: {}'.format(len(vocab_to_int)))\n",
    "else:\n",
    "    vocab_to_int = dict([(word, idx+1) for idx, word in enumerate(word_set)])\n",
    "    json.dump(vocab_to_int, open('vocab_to_int.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 2 #starting with lstm layer of 1 and if underfitting will expand to 2 \n",
    "batch_size = 1\n",
    "learning_rate = 0.001\n",
    "embed_size = 300\n",
    "seq_len = 200\n",
    "n_words = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, shape=(None, seq_len), name='inputs_')\n",
    "    labels_ = tf.placeholder(tf.int32, shape=(None, 1), name='labels_') #this might be wrong\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    with tf.name_scope('embedding_layer'):\n",
    "\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words + 1, embed_size), -1.0, 1.0), name='embedding') \n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "\n",
    "    with tf.name_scope('lstm'):\n",
    "\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_units = lstm_size)\n",
    "\n",
    "        #wraps lstm cell in a cell that applies dropout to the output of the lstm cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) \n",
    "    \n",
    "    with tf.name_scope('lstm_layer'):\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers) \n",
    "        initial_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "    with tf.name_scope('dynamic_rnn'):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell = cell, inputs = embed, initial_state=initial_state)\n",
    "   \n",
    "    with tf.name_scope('prediction'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], num_outputs=1, activation_fn=tf.sigmoid) \n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vocab_to_int,  size: 74072\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "if os.path.exists('vocab_to_int.json'):\n",
    "    vocab_to_int = json.load(open('vocab_to_int.json', 'r'))\n",
    "    print('loaded vocab_to_int,  size: {}'.format(len(vocab_to_int)))\n",
    "else:\n",
    "    vocab_to_int = dict([(word, idx+1) for idx, word in enumerate(word_set)])\n",
    "    json.dump(vocab_to_int, open('vocab_to_int.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "reviews =reviews.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "\n",
    "def analyze_sentence(sent1):\n",
    "    sent1 = ''.join([c for c in sent1.lower() if c not in punctuation])\n",
    "    words = sent1.split()\n",
    "\n",
    "    sub_sents = []\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        sub_sents.append(words[:i])\n",
    "    print(sub_sents[-1])\n",
    "    #skip words not in corpus\n",
    "    sub_sents_ints = [[vocab_to_int[word] for word in sent if word in vocab_to_int] for sent in sub_sents]\n",
    "\n",
    "    zero200_padding = lambda inp_list : [0 for _ in range(200 - len(inp_list))] + inp_list\n",
    "\n",
    "    prior_out = 0.0\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        saver.restore(sess, './checkpoints/sentiment.ckpt-11')\n",
    "        for i in range(min(seq_len, len(sub_sents_ints))):\n",
    "            out = sess.run(predictions, feed_dict={inputs_:np.array([zero200_padding(sub_sents_ints[i])]),\n",
    "                                         keep_prob: 1.0\n",
    "        #                                 initial_state: init_state\n",
    "                                        })\n",
    "            #print(out[0][0])\n",
    "            score = out[0, 0]\n",
    "            print('{:.3f}; {:.2f}: \\t {} \\t {}'.format(score, score - prior_out, 'positive' if out > 0.5 else 'negative', words[i]))\n",
    "            \n",
    "            prior_out = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'watched', 'this', 'movie', 'yesterday', 'and', 'thought', 'it', 'was', 'greatest', 'romantic', 'comedy', 'i', 'have', 'ever', 'seen', 'in', 'all', 'of', 'my', 'days', 'i', 'have', 'never', 'enjoyed', 'a', 'movie', 'as', 'much', 'as', 'ive', 'enjoyed', 'this', 'one', 'and', 'hope', 'everyone', 'will', 'have', 'a', 'chance', 'to', 'see', 'it']\n",
      "0.198; 0.20: \t negative \t i\n",
      "0.176; -0.02: \t negative \t watched\n",
      "0.125; -0.05: \t negative \t this\n",
      "0.111; -0.01: \t negative \t movie\n",
      "0.096; -0.01: \t negative \t yesterday\n",
      "0.096; -0.00: \t negative \t and\n",
      "0.120; 0.02: \t negative \t thought\n",
      "0.154; 0.03: \t negative \t it\n",
      "0.143; -0.01: \t negative \t was\n",
      "0.204; 0.06: \t negative \t greatest\n",
      "0.339; 0.13: \t negative \t romantic\n",
      "0.442; 0.10: \t negative \t comedy\n",
      "0.533; 0.09: \t positive \t i\n",
      "0.584; 0.05: \t positive \t have\n",
      "0.559; -0.02: \t positive \t ever\n",
      "0.677; 0.12: \t positive \t seen\n",
      "0.675; -0.00: \t positive \t in\n",
      "0.667; -0.01: \t positive \t all\n",
      "0.630; -0.04: \t positive \t of\n",
      "0.617; -0.01: \t positive \t my\n",
      "0.691; 0.07: \t positive \t days\n",
      "0.701; 0.01: \t positive \t i\n",
      "0.720; 0.02: \t positive \t have\n",
      "0.750; 0.03: \t positive \t never\n",
      "0.871; 0.12: \t positive \t enjoyed\n",
      "0.893; 0.02: \t positive \t a\n",
      "0.916; 0.02: \t positive \t movie\n",
      "0.918; 0.00: \t positive \t as\n",
      "0.901; -0.02: \t positive \t much\n",
      "0.887; -0.01: \t positive \t as\n",
      "0.896; 0.01: \t positive \t ive\n",
      "0.941; 0.05: \t positive \t enjoyed\n",
      "0.919; -0.02: \t positive \t this\n",
      "0.905; -0.01: \t positive \t one\n",
      "0.904; -0.00: \t positive \t and\n",
      "0.939; 0.03: \t positive \t hope\n",
      "0.959; 0.02: \t positive \t everyone\n",
      "0.966; 0.01: \t positive \t will\n",
      "0.968; 0.00: \t positive \t have\n",
      "0.966; -0.00: \t positive \t a\n",
      "0.976; 0.01: \t positive \t chance\n",
      "0.976; -0.00: \t positive \t to\n",
      "0.980; 0.00: \t positive \t see\n",
      "0.981; 0.00: \t positive \t it\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"\"\"\n",
    "i watched this movie yesterday and thought it was greatest romantic comedy i have ever seen. \n",
    "In all of my days i have never enjoyed a movie as much as i've enjoyed this one and hope everyone will have a chance to see it.\n",
    "\"\"\"\n",
    "analyze_sentence(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hate', 'this', 'movie', 'it', 'has', 'steven', 'segal', 'in', 'it', 'who', 'is', 'terrible', 'and', 'most', 'of', 'all', 'action', 'movies', 'should', 'only', 'have', 'keanu', 'reeves', 'id', 'rather', 'watch', 'two', 'rats', 'mating', 'than', 'have', 'to', 'see', 'this', 'again']\n",
      "0.198; 0.20: \t negative \t i\n",
      "0.180; -0.02: \t negative \t hate\n",
      "0.140; -0.04: \t negative \t this\n",
      "0.129; -0.01: \t negative \t movie\n",
      "0.142; 0.01: \t negative \t it\n",
      "0.145; 0.00: \t negative \t has\n",
      "0.134; -0.01: \t negative \t steven\n",
      "0.118; -0.02: \t negative \t segal\n",
      "0.121; 0.00: \t negative \t in\n",
      "0.144; 0.02: \t negative \t it\n",
      "0.140; -0.00: \t negative \t who\n",
      "0.134; -0.01: \t negative \t is\n",
      "0.078; -0.06: \t negative \t terrible\n",
      "0.087; 0.01: \t negative \t and\n",
      "0.105; 0.02: \t negative \t most\n",
      "0.122; 0.02: \t negative \t of\n",
      "0.122; -0.00: \t negative \t all\n",
      "0.143; 0.02: \t negative \t action\n",
      "0.212; 0.07: \t negative \t movies\n",
      "0.254; 0.04: \t negative \t should\n",
      "0.188; -0.07: \t negative \t only\n",
      "0.153; -0.04: \t negative \t have\n",
      "0.137; -0.02: \t negative \t keanu\n",
      "0.104; -0.03: \t negative \t reeves\n",
      "0.140; 0.04: \t negative \t id\n",
      "0.163; 0.02: \t negative \t rather\n",
      "0.212; 0.05: \t negative \t watch\n",
      "0.300; 0.09: \t negative \t two\n",
      "0.210; -0.09: \t negative \t rats\n",
      "0.296; 0.09: \t negative \t mating\n",
      "0.334; 0.04: \t negative \t than\n",
      "0.344; 0.01: \t negative \t have\n",
      "0.344; -0.00: \t negative \t to\n",
      "0.500; 0.16: \t negative \t see\n",
      "0.351; -0.15: \t negative \t this\n",
      "0.376; 0.02: \t negative \t again\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"\"\"\n",
    "I hate this movie. It has steven segal in it, who is terrible and most of all action movies should only have keanu reeves.\n",
    "I'd rather watch two rats mating than have to see this again.\n",
    "\"\"\"\n",
    "analyze_sentence(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'single', 'greatest', 'film', 'of', 'all', 'time', 'i', 'cant', 'wait', 'until', 'they', 'make', 'a', 'sequel']\n",
      "0.176; 0.18: \t negative \t this\n",
      "0.155; -0.02: \t negative \t is\n",
      "0.157; 0.00: \t negative \t the\n",
      "0.167; 0.01: \t negative \t single\n",
      "0.227; 0.06: \t negative \t greatest\n",
      "0.292; 0.07: \t negative \t film\n",
      "0.353; 0.06: \t negative \t of\n",
      "0.366; 0.01: \t negative \t all\n",
      "0.342; -0.02: \t negative \t time\n",
      "0.398; 0.06: \t negative \t i\n",
      "0.388; -0.01: \t negative \t cant\n",
      "0.536; 0.15: \t positive \t wait\n",
      "0.566; 0.03: \t positive \t until\n",
      "0.635; 0.07: \t positive \t they\n",
      "0.572; -0.06: \t positive \t make\n",
      "0.593; 0.02: \t positive \t a\n",
      "0.616; 0.02: \t positive \t sequel\n"
     ]
    }
   ],
   "source": [
    "analyze_sentence('this is the single greatest film of all time i cant wait until they make a sequel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'scene', 'that', 'just', 'doesnt', 'work', 'because', 'its', 'too', 'heavily', 'burdened', 'with', 'its', 'purpose', 'occurs', 'when', 'mcmurphy', 'escapes', 'commandeers', 'a', 'school', 'bus', 'and', 'takes', 'all', 'the', 'inmates', 'of', 'the', 'ward', 'on', 'a', 'fishing', 'trip', 'in', 'a', 'stolen', 'boat', 'the', 'scene', 'causes', 'an', 'almost', 'embarrassing', 'break', 'in', 'the', 'movie', 'its', 'formans', 'first', 'serious', 'misstep', 'because', 'its', 'an', 'idealized', 'fantasy', 'in', 'the', 'midst', 'of', 'realism', 'by', 'now', 'weve', 'met', 'the', 'characters', 'we', 'know', 'them', 'in', 'the', 'context', 'of', 'hospital', 'politics', 'and', 'when', 'theyre', 'set', 'down', 'on', 'the', 'boat', 'deck', 'they', 'just', 'dont', 'belong', 'there', 'the', 'ward', 'is', 'the', 'arena', 'in', 'which', 'theyll', 'win', 'or', 'lose', 'and', 'its', 'not', 'playing', 'fair', 'to', 'them', 'as', 'characters', 'to', 'give', 'them', 'a', 'fishing', 'trip', 'even', 'as', 'im', 'making', 'these', 'observations', 'though', 'i', 'cant', 'get', 'out', 'of', 'my', 'mind', 'the', 'tumultuous', 'response', 'that', 'cuckoos', 'nest', 'received', 'from', 'its', 'original', 'audiences', 'even', 'the', 'most', 'obvious', 'necessary', 'and', 'sobering', 'scenes', 'as', 'when', 'mcmurphy', 'tries', 'to', 'strangle', 'nurse', 'ratched', 'to', 'death', 'were', 'received', 'not', 'seriously', 'but', 'with', 'sophomoric', 'cheers', 'and', 'applause', 'maybe', 'thats', 'the', 'way', 'to', 'get', 'the', 'most', 'out', 'of', 'the', 'movie', 'see', 'it', 'as', 'a', 'simpleminded', 'antiestablishment', 'parable', 'but', 'i', 'hope', 'not', 'i', 'think', 'there', 'are', 'long', 'stretches', 'of', 'a', 'very', 'good', 'film', 'to', 'be', 'found', 'in', 'the', 'midst', 'of', 'formans', 'ultimate', 'failure', 'and', 'i', 'hope', 'they', 'dont', 'get', 'drowned', 'in', 'the', 'applause', 'for', 'the', 'bad', 'stuff', 'that', 'plays', 'to', 'the', 'galleries']\n",
      "0.191; 0.19: \t negative \t another\n",
      "0.175; -0.02: \t negative \t scene\n",
      "0.145; -0.03: \t negative \t that\n",
      "0.125; -0.02: \t negative \t just\n",
      "0.127; 0.00: \t negative \t doesnt\n",
      "0.141; 0.01: \t negative \t work\n",
      "0.136; -0.01: \t negative \t because\n",
      "0.155; 0.02: \t negative \t its\n",
      "0.193; 0.04: \t negative \t too\n",
      "0.205; 0.01: \t negative \t heavily\n",
      "0.236; 0.03: \t negative \t burdened\n",
      "0.274; 0.04: \t negative \t with\n",
      "0.344; 0.07: \t negative \t its\n",
      "0.514; 0.17: \t positive \t purpose\n",
      "0.533; 0.02: \t positive \t occurs\n",
      "0.549; 0.02: \t positive \t when\n",
      "0.613; 0.06: \t positive \t mcmurphy\n",
      "0.599; -0.01: \t positive \t escapes\n",
      "0.599; 0.00: \t positive \t commandeers\n",
      "0.634; 0.03: \t positive \t a\n",
      "0.638; 0.00: \t positive \t school\n",
      "0.607; -0.03: \t positive \t bus\n",
      "0.619; 0.01: \t positive \t and\n",
      "0.641; 0.02: \t positive \t takes\n",
      "0.638; -0.00: \t positive \t all\n",
      "0.634; -0.00: \t positive \t the\n",
      "0.584; -0.05: \t positive \t inmates\n",
      "0.553; -0.03: \t positive \t of\n",
      "0.577; 0.02: \t positive \t the\n",
      "0.695; 0.12: \t positive \t ward\n",
      "0.694; -0.00: \t positive \t on\n",
      "0.704; 0.01: \t positive \t a\n",
      "0.710; 0.01: \t positive \t fishing\n",
      "0.700; -0.01: \t positive \t trip\n",
      "0.664; -0.04: \t positive \t in\n",
      "0.673; 0.01: \t positive \t a\n",
      "0.604; -0.07: \t positive \t stolen\n",
      "0.432; -0.17: \t negative \t boat\n",
      "0.383; -0.05: \t negative \t the\n",
      "0.388; 0.01: \t negative \t scene\n",
      "0.433; 0.05: \t negative \t causes\n",
      "0.478; 0.04: \t negative \t an\n",
      "0.555; 0.08: \t positive \t almost\n",
      "0.475; -0.08: \t negative \t embarrassing\n",
      "0.311; -0.16: \t negative \t break\n",
      "0.235; -0.08: \t negative \t in\n",
      "0.222; -0.01: \t negative \t the\n",
      "0.223; 0.00: \t negative \t movie\n",
      "0.248; 0.03: \t negative \t its\n",
      "0.240; -0.01: \t negative \t formans\n",
      "0.210; -0.03: \t negative \t first\n",
      "0.205; -0.01: \t negative \t serious\n",
      "0.201; -0.00: \t negative \t misstep\n",
      "0.193; -0.01: \t negative \t because\n",
      "0.214; 0.02: \t negative \t its\n",
      "0.226; 0.01: \t negative \t an\n",
      "0.232; 0.01: \t negative \t idealized\n",
      "0.222; -0.01: \t negative \t fantasy\n",
      "0.202; -0.02: \t negative \t in\n",
      "0.195; -0.01: \t negative \t the\n",
      "0.246; 0.05: \t negative \t midst\n",
      "0.287; 0.04: \t negative \t of\n",
      "0.319; 0.03: \t negative \t realism\n",
      "0.323; 0.00: \t negative \t by\n",
      "0.325; 0.00: \t negative \t now\n",
      "0.347; 0.02: \t negative \t weve\n",
      "0.338; -0.01: \t negative \t met\n",
      "0.339; 0.00: \t negative \t the\n",
      "0.326; -0.01: \t negative \t characters\n",
      "0.296; -0.03: \t negative \t we\n",
      "0.289; -0.01: \t negative \t know\n",
      "0.262; -0.03: \t negative \t them\n",
      "0.225; -0.04: \t negative \t in\n",
      "0.217; -0.01: \t negative \t the\n",
      "0.188; -0.03: \t negative \t context\n",
      "0.172; -0.02: \t negative \t of\n",
      "0.147; -0.03: \t negative \t hospital\n",
      "0.138; -0.01: \t negative \t politics\n",
      "0.142; 0.00: \t negative \t and\n",
      "0.141; -0.00: \t negative \t when\n",
      "0.128; -0.01: \t negative \t theyre\n",
      "0.121; -0.01: \t negative \t set\n",
      "0.110; -0.01: \t negative \t down\n",
      "0.089; -0.02: \t negative \t on\n",
      "0.092; 0.00: \t negative \t the\n",
      "0.077; -0.02: \t negative \t boat\n",
      "0.068; -0.01: \t negative \t deck\n",
      "0.071; 0.00: \t negative \t they\n",
      "0.071; 0.00: \t negative \t just\n",
      "0.066; -0.00: \t negative \t dont\n",
      "0.057; -0.01: \t negative \t belong\n",
      "0.050; -0.01: \t negative \t there\n",
      "0.051; 0.00: \t negative \t the\n",
      "0.053; 0.00: \t negative \t ward\n",
      "0.056; 0.00: \t negative \t is\n",
      "0.065; 0.01: \t negative \t the\n",
      "0.070; 0.01: \t negative \t arena\n",
      "0.073; 0.00: \t negative \t in\n",
      "0.072; -0.00: \t negative \t which\n",
      "0.072; 0.00: \t negative \t theyll\n",
      "0.079; 0.01: \t negative \t win\n",
      "0.076; -0.00: \t negative \t or\n",
      "0.088; 0.01: \t negative \t lose\n",
      "0.109; 0.02: \t negative \t and\n",
      "0.135; 0.03: \t negative \t its\n",
      "0.125; -0.01: \t negative \t not\n",
      "0.116; -0.01: \t negative \t playing\n",
      "0.108; -0.01: \t negative \t fair\n",
      "0.106; -0.00: \t negative \t to\n",
      "0.095; -0.01: \t negative \t them\n",
      "0.087; -0.01: \t negative \t as\n",
      "0.079; -0.01: \t negative \t characters\n",
      "0.080; 0.00: \t negative \t to\n",
      "0.077; -0.00: \t negative \t give\n",
      "0.069; -0.01: \t negative \t them\n",
      "0.071; 0.00: \t negative \t a\n",
      "0.066; -0.00: \t negative \t fishing\n",
      "0.060; -0.01: \t negative \t trip\n",
      "0.052; -0.01: \t negative \t even\n",
      "0.050; -0.00: \t negative \t as\n",
      "0.049; -0.00: \t negative \t im\n",
      "0.047; -0.00: \t negative \t making\n",
      "0.044; -0.00: \t negative \t these\n",
      "0.046; 0.00: \t negative \t observations\n",
      "0.042; -0.00: \t negative \t though\n",
      "0.046; 0.00: \t negative \t i\n",
      "0.044; -0.00: \t negative \t cant\n",
      "0.041; -0.00: \t negative \t get\n",
      "0.043; 0.00: \t negative \t out\n",
      "0.045; 0.00: \t negative \t of\n",
      "0.052; 0.01: \t negative \t my\n",
      "0.054; 0.00: \t negative \t mind\n",
      "0.061; 0.01: \t negative \t the\n",
      "0.057; -0.00: \t negative \t tumultuous\n",
      "0.061; 0.00: \t negative \t response\n",
      "0.063; 0.00: \t negative \t that\n",
      "0.067; 0.00: \t negative \t cuckoos\n",
      "0.069; 0.00: \t negative \t nest\n",
      "0.078; 0.01: \t negative \t received\n",
      "0.077; -0.00: \t negative \t from\n",
      "0.083; 0.01: \t negative \t its\n",
      "0.079; -0.00: \t negative \t original\n",
      "0.062; -0.02: \t negative \t audiences\n",
      "0.052; -0.01: \t negative \t even\n",
      "0.056; 0.00: \t negative \t the\n",
      "0.057; 0.00: \t negative \t most\n",
      "0.059; 0.00: \t negative \t obvious\n",
      "0.062; 0.00: \t negative \t necessary\n",
      "0.065; 0.00: \t negative \t and\n",
      "0.063; -0.00: \t negative \t sobering\n",
      "0.062; -0.00: \t negative \t scenes\n",
      "0.060; -0.00: \t negative \t as\n",
      "0.055; -0.00: \t negative \t when\n",
      "0.058; 0.00: \t negative \t mcmurphy\n",
      "0.054; -0.00: \t negative \t tries\n",
      "0.055; 0.00: \t negative \t to\n",
      "0.049; -0.01: \t negative \t strangle\n",
      "0.047; -0.00: \t negative \t nurse\n",
      "0.047; 0.00: \t negative \t ratched\n",
      "0.050; 0.00: \t negative \t to\n",
      "0.048; -0.00: \t negative \t death\n",
      "0.047; -0.00: \t negative \t were\n",
      "0.050; 0.00: \t negative \t received\n",
      "0.045; -0.01: \t negative \t not\n",
      "0.038; -0.01: \t negative \t seriously\n",
      "0.040; 0.00: \t negative \t but\n",
      "0.038; -0.00: \t negative \t with\n",
      "0.036; -0.00: \t negative \t sophomoric\n",
      "0.039; 0.00: \t negative \t cheers\n",
      "0.043; 0.00: \t negative \t and\n",
      "0.051; 0.01: \t negative \t applause\n",
      "0.053; 0.00: \t negative \t maybe\n",
      "0.045; -0.01: \t negative \t thats\n",
      "0.048; 0.00: \t negative \t the\n",
      "0.047; -0.00: \t negative \t way\n",
      "0.050; 0.00: \t negative \t to\n",
      "0.048; -0.00: \t negative \t get\n",
      "0.052; 0.00: \t negative \t the\n",
      "0.055; 0.00: \t negative \t most\n",
      "0.062; 0.01: \t negative \t out\n",
      "0.069; 0.01: \t negative \t of\n",
      "0.077; 0.01: \t negative \t the\n",
      "0.086; 0.01: \t negative \t movie\n",
      "0.103; 0.02: \t negative \t see\n",
      "0.122; 0.02: \t negative \t it\n",
      "0.134; 0.01: \t negative \t as\n",
      "0.156; 0.02: \t negative \t a\n",
      "0.153; -0.00: \t negative \t simpleminded\n",
      "0.153; 0.00: \t negative \t antiestablishment\n",
      "0.135; -0.02: \t negative \t parable\n",
      "0.132; -0.00: \t negative \t but\n",
      "0.135; 0.00: \t negative \t i\n",
      "0.156; 0.02: \t negative \t hope\n",
      "0.135; -0.02: \t negative \t not\n",
      "0.128; -0.01: \t negative \t i\n",
      "0.127; -0.00: \t negative \t think\n",
      "0.113; -0.01: \t negative \t there\n",
      "0.112; -0.00: \t negative \t are\n",
      "0.110; -0.00: \t negative \t long\n",
      "0.106; -0.00: \t negative \t stretches\n"
     ]
    }
   ],
   "source": [
    "# a some what negative review of one flew over the cuckoos nest\n",
    "#http://www.rogerebert.com/reviews/one-flew-over-the-cuckoos-nest-1975\n",
    "\n",
    "sent = \"\"\"\n",
    "Another scene that just doesn't work, because it's too heavily burdened with its purpose, occurs when McMurphy escapes, commandeers a school bus, and takes all the inmates of the ward on a fishing trip in a stolen boat. The scene causes an almost embarrassing break in the movie -- it's Forman's first serious misstep -- because it's an idealized fantasy in the midst of realism. By now, we've met the characters, we know them in the context of hospital politics, and when they're set down on the boat deck, they just don't belong there. The ward is the arena in which they'll win or lose, and it's not playing fair -- to them, as characters -- to give them a fishing trip.\n",
    "Even as I'm making these observations, though, I can't get out of my mind the tumultuous response that \"Cuckoo's Nest\" received from its original audiences. Even the most obvious, necessary, and sobering scenes -- as when McMurphy tries to strangle Nurse Ratched to death -- were received, not seriously, but with sophomoric cheers and applause. Maybe that's the way to get the most out of the movie -- see it as a simple-minded antiestablishment parable -- but I hope not. I think there are long stretches of a very good film to be found in the midst of Forman's ultimate failure, and I hope they don't get drowned in the applause for the bad stuff that plays to the galleries.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "analyze_sentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
