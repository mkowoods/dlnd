{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sentimet RNN\n",
    "\n",
    "Dataset: imdb movie reviews and labels(pos/neg).\n",
    "\n",
    "A RNN used to predict the sentiment from the words of a review. The model uses an embedding layer to map the words from the review to embedding and the those embeddings are passed to several LSTM cells, which will output to a sigmoid that will predict the sentiment.\n",
    "\n",
    "Input: word -> one_hot_encoding_of_word -> embedding -> LSTM Recurrent Cells -> Output: Sigmoid\n",
    "\n",
    "----------\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./reviews.txt', 'r') as f:\n",
    "    reviews = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./labels.txt', 'r') as f:\n",
    "    labels = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "reviews = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = reviews.split('\\n') #get rid of the \\n character\n",
    "\n",
    "word_set = set([word for word in (' '.join(reviews)).split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grouping', 'novellas', 'osiris', 'ruling', 'diagonal', 'entails', 'swanks', 'romulus', 'morissey', 'brillent', 'artworks', 'juries', 'teleprinter', 'adage', 'eschew', 'enclosed', 'teta', 'devotee', 'lune', 'weixler', 'section', 'silences', 'whistlestop', 'malozzie', 'bloss']\n"
     ]
    }
   ],
   "source": [
    "print(list(word_set)[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when i come on imdb boards  i  m always fed up when i see a  the worst movie ever  post  after watching this movie  i think that i am soon going to create my own post    br    br   the opening titlesgreat  some kind of lame zoom on a gas oven  yeah  focus on the fireexplosionsgreat action packed movie     br    br   the actorsi think that ice t is a cool rapper  even a nice actor  sometimes  i insist   sometimes   but the steven seagal like policeman he plays is    beyond the words  the rest of the cast is    well i don  t know where those actors were hired but jeez   i bet my dog would have been a much better actor than them    br    br   the plothijacking  original isn  t it    br    br   the action sequencesthe first shot of the movie is an explosion  i told myself  well  cool   at least there will be some nice pyrotechnics    i was dead wrong  the rest of the movie is mostly filled with low rent stock shots taken from the air force     br    br   the dialogs are hilarious  the music is pure crap  the end is happy  i mean i was happy at the end because the movie was over      br    br   my cousin who was watching the movie was delighted  i  m    she  s      well   i was on the verge of taking the movie and burn it  maybe next time i  m gonna watch it     who said never      \n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "    if 'sequencesthe' in review:\n",
    "        print (review)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Encoding words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "<b>Exercise:</b> Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers start at 1, not 0. Also, convert the reviews to integers and store the reviews in a new list called reviews_ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded vocab_to_int,  size: 74072\n"
     ]
    }
   ],
   "source": [
    "#saving mapping table\n",
    "import json\n",
    "import os\n",
    "if os.path.exists('vocab_to_int.json'):\n",
    "    vocab_to_int = json.load(open('vocab_to_int.json', 'r'))\n",
    "    print('loaded vocab_to_int,  size: {}'.format(len(vocab_to_int)))\n",
    "else:\n",
    "    vocab_to_int = dict([(word, idx+1) for idx, word in enumerate(word_set)])\n",
    "    json.dump(vocab_to_int, open('vocab_to_int.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews_ints = [[vocab_to_int[word] for word in review.split()] for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = [1 if label == 'positive' else 0 for label in labels if len(label) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero length reviews: 1\n",
      "Maximum length reviews: 2514\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum length reviews: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Okay, a couple issues here. We seem to have one review with zero length. And, the maximum review length is way too many steps for our RNN. Let's truncate to 200 steps. For reviews shorter than 200, we'll pad with 0s. For reviews longer than 200, we can truncate them to the first 200 characters.\n",
    "\n",
    "Exercise: First, remove the review with zero length from the reviews_ints list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews_ints = [rev for rev in reviews_ints if len(rev) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<b>Exercise:</b> Now, create an array features that contains the data we'll pass to the network. The data should come from review_ints, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is ['best', 'movie', 'ever'], [117, 18, 128] as integers, the row will look like [0, 0, 0, ..., 0, 117, 18, 128]. For reviews longer than 200, use on the first 200 words as the feature vector.\n",
    "\n",
    "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#not sure i buy why only 200 words makes sense, but following along with tthe tutorial\n",
    "seq_len = 200\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for idx, review in enumerate(reviews_ints):\n",
    "    trunc_rev = review[:seq_len]\n",
    "    start_idx = (seq_len - len(trunc_rev))\n",
    "    features[idx, start_idx:] = trunc_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 42386, 22123, 33565,\n",
       "        70291, 38773, 11818,  1901, 72811, 10903, 72356, 25002, 11449,\n",
       "        35077, 41756, 48831, 13275, 69853, 45314, 20860, 70669, 35077,\n",
       "        47666, 60116, 34992, 21234, 72356, 15004, 31694, 44112, 19702,\n",
       "        55762, 72405, 39159, 42386, 22123, 39374, 34820, 33565, 45612,\n",
       "        24616],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0, 42049, 48776, 70291, 73096,\n",
       "        37995, 15489, 55729, 25846, 67750, 70291, 57403, 26040,  7451,\n",
       "        54579],\n",
       "       [47648, 53166, 35386, 35077,  6850, 50916, 71972, 15489,  7851,\n",
       "        22700, 65094, 67750, 34992, 44870,   293, 70291, 38248, 55762,\n",
       "        38628, 73826, 16979, 72356, 12860, 39159, 16848, 60513, 30609,\n",
       "         8199, 37995,  1752, 45931, 40592, 19428, 55762, 45314, 37297,\n",
       "        53166, 63424, 67750, 72356, 17173, 21650, 72381, 36630, 48776,\n",
       "        72356, 20229, 35077, 46055, 70291, 39192, 12956, 51625, 36479,\n",
       "        69853, 72600, 70669, 35077, 16606, 72356, 22451, 16979, 44038,\n",
       "        23882,  1036, 55762, 36289,  3602, 72356, 20075,  8838, 53166,\n",
       "        36479, 65209, 67404, 51319, 32533, 42626, 55762, 23629, 14768,\n",
       "        16979, 72356, 43526, 68937, 68937, 44870, 67657, 65209,  3092,\n",
       "        16848, 58892, 70291, 55426, 55762, 65585, 16979, 72356, 43526,\n",
       "        67750],\n",
       "       [51721, 26040, 35077, 70291, 32904, 74061,  5871, 12670, 33565,\n",
       "        50732, 14768, 54579, 60781, 70747, 70669, 31452, 55762, 34917,\n",
       "        73101, 60707, 24490, 51071, 53843, 37995, 33565, 68514, 49421,\n",
       "        70291, 43187, 48776,  9815, 39374, 55762, 18440,  1525, 21234,\n",
       "        12220, 48776,  1901, 10589, 52329, 55762, 72356,  8282, 35077,\n",
       "        70291, 74043, 22885, 16979,  3594, 33565, 24490, 27483, 57494,\n",
       "        41810, 13391, 44730, 11015, 72356,  5871, 55897, 34279, 65576,\n",
       "        35077, 43578, 44870,    32, 19088, 72356, 12670, 33565, 14671,\n",
       "        24398, 17438, 72356, 11871, 26817, 60990, 10172, 31977, 18440,\n",
       "        51289,  5237, 39374, 26778, 49510, 45836, 60723, 16112, 23023,\n",
       "        37995, 34180, 72356, 11951, 49065,  7451, 54579, 39209, 19280,\n",
       "        67404],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0, 38524, 71450, 72245, 17438,\n",
       "        18466, 62513, 14795, 62097, 26589,   128, 35176, 41694, 43286,\n",
       "        50717, 16753, 25888, 44603,  5292, 21234, 49232, 37846, 42090,\n",
       "        59540, 55762,  6046, 72356, 12420, 16979, 19246, 33565, 70291,\n",
       "        55921, 35077, 36769, 35077, 71997, 21234, 37444, 13274, 72356,\n",
       "        43203, 16979, 23113, 33565, 22885, 62055, 63498, 10589, 65374,\n",
       "        48776],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "        31729, 17640, 48076, 52022, 41694, 11487,  7317, 52331, 60116,\n",
       "        66941, 16979, 10903, 27019,  3588, 16979, 72356,  2659, 48776,\n",
       "        72356, 54517, 70023, 31729,  2808, 63275, 55762,  8864, 48776,\n",
       "        19438, 21124, 57557, 61744, 72356, 72183, 54579, 44730, 54517,\n",
       "        73096, 13338, 72356, 47578,  5292,  4030, 65844, 35077, 10589,\n",
       "        40291],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0, 31729, 33565, 30113, 72356, 21650,\n",
       "        39489, 17640, 62258, 72356,  5373, 15173, 60967, 33581, 65290,\n",
       "         1901, 60587,  3649, 51675, 70291, 12457, 44302, 48776, 47648,\n",
       "        69710],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0, 38748,  4730, 41694, 57387, 31729, 33565,   447,\n",
       "        55762, 32533, 22700, 40229, 17640, 44870, 53803, 67404, 42650,\n",
       "        43286, 71316,  7451, 28613, 10903, 72356, 68286, 45540, 72381,\n",
       "        27329, 32973, 53030, 38413,  7451, 25888,  3649, 16452, 16071,\n",
       "        72356, 52585, 17659, 25888, 58487, 63719, 64981, 46098, 31729,\n",
       "        42049, 33565, 72049, 34525, 55762, 16452, 72356, 48852, 48776,\n",
       "        70291],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0, 31729, 33565,\n",
       "         3649, 72356, 22251,  9750,  5373, 17640,  1901, 64981, 45612,\n",
       "        72879, 49524,  9855, 21650, 48776, 18440, 18636, 25888, 68929,\n",
       "        49556],\n",
       "       [21124, 41694, 64981, 32692, 60116, 45042,  9240, 19702, 67014,\n",
       "        55762, 72356, 20569, 55762, 63463, 34275,  1901, 64981, 10355,\n",
       "        48776, 61752, 18636, 41694, 60045, 54579, 60116, 45042, 44870,\n",
       "        31729, 64981, 72356, 56259, 10355, 52555, 24446,  7451, 48776,\n",
       "        14059, 46726, 41694, 49556,   293, 16753, 34275, 62695, 46055,\n",
       "        31609, 25888, 41694, 27329, 43286, 30433,  7451, 72356, 30454,\n",
       "        48776, 60116, 20860, 64840,  1901, 67657, 70291, 21999, 50976,\n",
       "        25888, 24905, 25063, 34061, 48776, 39374, 16431, 25888, 62217,\n",
       "        62458,  2507, 73091, 33565, 10355, 48776, 60116, 12561, 19277,\n",
       "        44870, 34275, 33565, 17438, 49590, 72356, 15286, 34061, 48776,\n",
       "        49836, 48776, 18440,  5701, 21234, 72356,  7738, 44194, 48776,\n",
       "        27809]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10, :100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training, Validation, Test\n",
    "\n",
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "Exercise: Create the training, validation, and test sets here. You'll need to create sets for the features and the labels, train_x and train_y for example. Define a split fraction, split_frac as the fraction of data to keep in the training set. Usually this is set to 0.8 or 0.9. The rest of the data will be split in half to create the validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = labels.reshape(len(labels), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200), (20000, 1) \n",
      "Validation set: \t(2500, 200), (2500, 1) \n",
      "Test set: \t\t(2500, 200), (2500, 1)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "r,c = features.shape\n",
    "\n",
    "train_x, val_x = features[:int(r*split_frac)], features[int(r*split_frac):]\n",
    "train_y, val_y = labels[:int(r*split_frac)], labels[int(r*split_frac):]\n",
    "\n",
    "r_val, _ = val_x.shape\n",
    "\n",
    "val_x, test_x = val_x[:int(0.5 * r_val)], val_x[int(0.5 * r_val):]\n",
    "val_y, test_y = val_y[:int(0.5 * r_val)], val_y[int(0.5 * r_val):]\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}, {}\".format(train_x.shape, train_y.shape), \n",
    "      \"\\nValidation set: \\t{}, {}\".format(val_x.shape, val_y.shape),\n",
    "      \"\\nTest set: \\t\\t{}, {}\".format(test_x.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# Build the graph\n",
    "\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    " - lstm_size: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    " - lstm_layers: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    " - batch_size: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    " - learning_rate: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 2 #starting with lstm layer of 1 and if underfitting will expand to 2 \n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "For the network itself, we'll be passing in our 200 element long review vectors. Each batch will be batch_size vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability.\n",
    "\n",
    "Exercise: Create the inputs_, labels_, and drop out keep_prob placeholders using tf.placeholder. labels_ needs to be two-dimensional to work with some functions later. Since keep_prob is a scalar (a 0-dimensional tensor), you shouldn't provide a size to tf.placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_words: 74072\n"
     ]
    }
   ],
   "source": [
    "n_words = len(word_set)\n",
    "print('n_words: {}'.format(n_words))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, shape=(batch_size, seq_len), name='inputs_')\n",
    "    labels_ = tf.placeholder(tf.int32, shape=(batch_size, 1), name='labels_') #this might be wrong\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Embedding Layer\n",
    "\n",
    "Adding layer to map the ~74K word vocab to size 300 feature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('embedding_layer'):\n",
    "        #added 1 to embedding since 0th row is used for padding\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words + 1, embed_size), -1.0, 1.0), name='embedding') \n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "        tf.summary.histogram('embedding', embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM Cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('lstm'):\n",
    "        #creates an lstm cell with num_units = lstm_size\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_units = lstm_size)\n",
    "\n",
    "        #wraps lstm cell in a cell that applies dropout to the output of the lstm cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Note from tutorial\n",
    "\n",
    "Most of the time, you're network will have better performance with more layers. That's sort of the magic of deep learning, adding more layers allows the network to learn really complex relationships. Again, there is a simple way to create multiple layers of LSTM cells with tf.contrib.rnn.MultiRNNCell.\n",
    "\n",
    "Here, [drop] * lstm_layers creates a list of cells (drop) that is lstm_layers long. The MultiRNNCell wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "So the final cell you're using in the network is actually multiple (or just one) LSTM cells with dropout. But it all works the same from an achitectural viewpoint, just a more complicated graph in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('lstm_layer'):\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers) \n",
    "        initial_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RNN forward pass\n",
    "\n",
    "Now we need to actually run the data through the RNN nodes. You can use tf.nn.dynamic_rnn to do this. You'd pass in the RNN cell you created (our multiple layered LSTM cell for instance), and the inputs to the network.\n",
    "\n",
    "`outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)`\n",
    "\n",
    "Above I created an initial state, initial_state, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. tf.nn.dynamic_rnn takes care of most of the work for us. We pass in our cell and the input to the cell, then it does the unrolling and everything else for us. It returns outputs for each time step and the final_state of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('dynamic_rnn'):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell = cell, inputs = embed, initial_state=initial_state)\n",
    "        tf.summary.histogram('outputs', outputs)\n",
    "        tf.summary.histogram('final_state', final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## outputs\n",
    "\n",
    "we only care about the final output, `outputs[:, -1]`, which we'll compare to `labels_` to determine the cost of the rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('prediction'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], num_outputs=1, activation_fn=tf.sigmoid) \n",
    "        \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels=labels_, predictions=predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## validation accuracy\n",
    "\n",
    "add nodes to calculate validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size = 100):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches * batch_size], y[:n_batches * batch_size]\n",
    "    for idx in range(0, len(x), batch_size):\n",
    "        yield x[idx : idx + batch_size], y[idx : idx + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# write out graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "### Before running create a checkpoints directory and summary director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    epochs = 15\n",
    "    print(\"lstm_size: {}, lstm_layers: {}, batch_size: {}, learning_rate: {}\".format(lstm_size, lstm_layers, batch_size, learning_rate))\n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        graph_writer = tf.summary.FileWriter('./logs/graph', sess.graph)\n",
    "        train_writer = tf.summary.FileWriter('./logs/perf/train')\n",
    "        valid_writer = tf.summary.FileWriter('./logs/perf/valid')\n",
    "        iteration = 1\n",
    "\n",
    "        for e in range(epochs):\n",
    "            #print(e)\n",
    "            state = sess.run(initial_state)\n",
    "            for x,y in get_batches(train_x, train_y, batch_size):\n",
    "                #print(x,y)\n",
    "                feed = {inputs_: x,\n",
    "                        labels_: y,\n",
    "                        keep_prob: 0.5,\n",
    "                        initial_state: state}\n",
    "                loss, state, _, summary, acc = sess.run([cost, final_state, opt, merged, accuracy], feed_dict=feed)\n",
    "\n",
    "                if iteration%5 == 0:\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Train Loss: {:.3f}\".format(loss),\n",
    "                          \"Accuracy: {:.3f}\".format(acc)\n",
    "                         )\n",
    "                if iteration%25 == 0:\n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                    for vx, vy in get_batches(val_x, val_y, batch_size):\n",
    "                        feed = {inputs_: vx,\n",
    "                                labels_: vy,\n",
    "                                keep_prob: 1.0,\n",
    "                                initial_state: val_state\n",
    "                               }\n",
    "                        batch_acc, val_state, summary = sess.run([accuracy, final_state, merged], feed_dict=feed)\n",
    "                        val_acc.append(batch_acc)\n",
    "                    valid_writer.add_summary(summary, iteration)\n",
    "                    print('Validation Acc: {:.3f}'.format(np.mean(val_acc)))\n",
    "\n",
    "                iteration += 1\n",
    "            saver.save(sess, \"./checkpoints/sentiment.ckpt\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.809\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"./checkpoints\"))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for x,y in get_batches(test_x, test_y, batch_size):\n",
    "        feed_dict = {\n",
    "            inputs_: x,\n",
    "            labels_: y,\n",
    "            keep_prob: 1.0,\n",
    "            initial_state: test_state\n",
    "        }\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed_dict)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test Accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.821\n"
     ]
    }
   ],
   "source": [
    "#evidence of overfitting pass the 11th ckpt\n",
    "\n",
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, './checkpoints/sentiment.ckpt-11')\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for x,y in get_batches(test_x, test_y, batch_size):\n",
    "        feed_dict = {\n",
    "            inputs_: x,\n",
    "            labels_: y,\n",
    "            keep_prob: 1.0,\n",
    "            initial_state: test_state\n",
    "        }\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed_dict)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test Accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sent1 = 'i watched this movie yesterday and thought it was great hamburger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'watched'], ['i', 'watched', 'this'], ['i', 'watched', 'this', 'movie'], ['i', 'watched', 'this', 'movie', 'yesterday'], ['i', 'watched', 'this', 'movie', 'yesterday', 'and'], ['i', 'watched', 'this', 'movie', 'yesterday', 'and', 'thought'], ['i', 'watched', 'this', 'movie', 'yesterday', 'and', 'thought', 'it'], ['i', 'watched', 'this', 'movie', 'yesterday', 'and', 'thought', 'it', 'was'], ['i', 'watched', 'this', 'movie', 'yesterday', 'and', 'thought', 'it', 'was', 'great'], ['i', 'watched', 'this', 'movie', 'yesterday', 'and', 'thought', 'it', 'was', 'great', 'hamburger']]\n"
     ]
    }
   ],
   "source": [
    "sub_sents = []\n",
    "\n",
    "for i in range(2, len(words) + 1):\n",
    "    sub_sents.append(words[:i])\n",
    "print(sub_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_sents_ints = [[vocab_to_int[word] for word in sent] for sent in sub_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero200_padding = lambda inp_list : [0 for _ in range(200 - len(inp_list))] + inp_list"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
